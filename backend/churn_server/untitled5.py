# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LkKdy8i0HpCKhW6pu0uvCIk8chaoDwbe
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from statsmodels.tsa.statespace.sarimax import SARIMAX
import warnings

# 1. Setup
warnings.filterwarnings('ignore')
plt.style.use('ggplot')

# Path for Google Colab
FILE_PATH = "/content/telecom_33_towers_4_operators.csv"

def run_bootstrapped_signal_forecast(path):
    # 2. Load Data
    try:
        df = pd.read_csv(path)
    except FileNotFoundError:
        print("Error: Please upload the CSV to the Colab 'Files' folder.")
        return

    df['date'] = pd.to_datetime(df['date'])
    operators = df['operator'].unique()

    for op_name in operators:
        # Aggregate to Daily Average Signal Strength
        op_data = df[df['operator'] == op_name].groupby('date').agg({
            'signal_strength_dbm': 'mean',
            'is_weekend': 'max'
        }).asfreq('D').interpolate(method='time')

        # 3. SARIMAX Model (1,1,1)
        model = SARIMAX(
            op_data['signal_strength_dbm'],
            exog=op_data[['is_weekend']],
            order=(1,1,1),
            seasonal_order=(1,1,1,7),
            enforce_stationarity=False,
            enforce_invertibility=False
        )
        results = model.fit(disp=False)

        # 4. TRAINING: Use Actual Historical Errors
        # This makes the 'Train' part an EXACT match for your real messy data.
        fitted_smooth = results.fittedvalues
        historical_errors = results.resid.values[2:] # Skip warm-up errors
        realistic_train = fitted_smooth[2:] + historical_errors

        # 5. PREDICTION: Bootstrapping Alternative
        forecast_steps = 60
        future_dates = pd.date_range(start=op_data.index[-1] + pd.Timedelta(days=1), periods=forecast_steps)
        future_exog = pd.DataFrame({
            'is_weekend': [1 if d.weekday() >= 5 else 0 for d in future_dates]
        }, index=future_dates)

        forecast_obj = results.get_forecast(steps=forecast_steps, exog=future_exog)
        forecast_smooth = forecast_obj.predicted_mean

        # --- ALTERNATIVE NOISE METHOD: BOOTSTRAPPING ---
        # Instead of np.random.normal, we pick random samples from the actual past errors
        bootstrapped_noise = np.random.choice(historical_errors, size=forecast_steps, replace=True)
        realistic_test = forecast_smooth + bootstrapped_noise
        # -----------------------------------------------

        # 6. Visualization
        plt.figure(figsize=(15, 6))

        # Plot Realistic History (Past) - Teal
        plt.plot(op_data.index[2:], realistic_train, color='#16a085',
                 label='Realistic Train (Actual History)', alpha=0.8)

        # Plot Realistic Forecast (Future) - Red
        plt.plot(future_dates, realistic_test, color='#c0392b',
                 label='Realistic 60-Day Forecast (Bootstrapped)', linewidth=1.5)

        # Plot the Trend Line - Dashed Black
        plt.plot(future_dates, forecast_smooth, color='black',
                 linestyle='--', label='Statistical Trend Logic', alpha=0.4)

        # Shaded Confidence Interval (from original model)
        conf_int = forecast_obj.conf_int()
        plt.fill_between(future_dates, conf_int.iloc[:, 0], conf_int.iloc[:, 1],
                         color='#c0392b', alpha=0.1, label='Confidence Range')

        # Formatting
        plt.title(f'Realistic Signal Strength Forecast: {op_name} (Using Residual Bootstrapping)', fontsize=15)
        plt.ylabel('Signal Strength (dBm)')
        plt.xlabel('Date')
        plt.axvline(op_data.index[-1], color='black', linestyle='-', alpha=0.5) # Today
        plt.legend(loc='upper left', ncol=2)
        plt.grid(True, alpha=0.2)
        plt.tight_layout()

        plt.show()

if __name__ == "__main__":
    run_bootstrapped_signal_forecast(FILE_PATH)

import pandas as pd
import numpy as np
from statsmodels.tsa.statespace.sarimax import SARIMAX
import warnings

# 1. Setup
warnings.filterwarnings('ignore')

# Use the path for your Google Colab environment
FILE_PATH = "/content/telecom_33_towers_4_operators.csv"

def get_forecast_values(path):
    # 2. Load Data
    try:
        df = pd.read_csv(path)
    except FileNotFoundError:
        print("Error: Please upload the CSV to the Colab 'Files' folder.")
        return

    df['date'] = pd.to_datetime(df['date'])
    operators = df['operator'].unique()
    all_forecasts = []

    print("Generating predictions for each operator...")

    for op_name in operators:
        # Aggregate to Daily Average Signal Strength
        op_data = df[df['operator'] == op_name].groupby('date').agg({
            'signal_strength_dbm': 'mean',
            'is_weekend': 'max'
        }).asfreq('D').interpolate(method='time')

        # 3. Fit the SARIMAX Model (1,1,1)
        model = SARIMAX(
            op_data['signal_strength_dbm'],
            exog=op_data[['is_weekend']],
            order=(1,1,1),
            seasonal_order=(1,1,1,7),
            enforce_stationarity=False,
            enforce_invertibility=False
        )
        results = model.fit(disp=False)

        # 4. Extract Historical Errors (Residuals)
        historical_errors = results.resid.values[2:]

        # 5. Predict for 60 Days
        forecast_steps = 60
        future_dates = pd.date_range(start=op_data.index[-1] + pd.Timedelta(days=1), periods=forecast_steps)
        future_exog = pd.DataFrame({
            'is_weekend': [1 if d.weekday() >= 5 else 0 for d in future_dates]
        }, index=future_dates)

        forecast_obj = results.get_forecast(steps=forecast_steps, exog=future_exog)
        forecast_smooth = forecast_obj.predicted_mean

        # Apply Bootstrapping Noise (Realistic spikes)
        bootstrapped_noise = np.random.choice(historical_errors, size=forecast_steps, replace=True)
        realistic_values = forecast_smooth + bootstrapped_noise

        # 6. Build the result table for this operator
        op_df = pd.DataFrame({
            'Date': future_dates.strftime('%Y-%m-%d'),
            'Operator': op_name,
            'Predicted_Signal_dBm': realistic_values.values.round(2)
        })
        all_forecasts.append(op_df)

    # 7. Combine and Save
    final_table = pd.concat(all_forecasts, ignore_index=True)
    final_table.to_csv("predicted_signal_values.csv", index=False)

    # 8. Display Results
    print("\n--- PREDICTED VALUES (First 5 days for each operator) ---")
    for op in operators:
        print(f"\nOperator: {op}")
        print(final_table[final_table['Operator'] == op].head(5).to_string(index=False))

    print(f"\nDone! Full 60-day results for all operators saved to: predicted_signal_values.csv")
    return final_table

# Run the function
forecast_data = get_forecast_values(FILE_PATH)

import pandas as pd
import numpy as np
from statsmodels.tsa.statespace.sarimax import SARIMAX
import warnings

# 1. Setup
warnings.filterwarnings('ignore')

# Use the path for your environment (Colab or Local)
FILE_PATH = "/content/telecom_33_towers_4_operators.csv"

def generate_area_operator_forecast(path):
    # 2. Load Data
    try:
        df = pd.read_csv(path)
    except FileNotFoundError:
        print("Error: File not found. Please check the path or upload the CSV.")
        return

    df['date'] = pd.to_datetime(df['date'])

    # Identify unique areas and operators
    cities = df['city'].unique()
    operators = df['operator'].unique()

    all_results = []

    print(f"Starting area-based forecasting for {len(cities)} cities and {len(operators)} operators...")

    for city in cities:
        for op in operators:
            # 3. Filter data for the specific Area and Operator
            subset = df[(df['city'] == city) & (df['operator'] == op)]

            if len(subset) < 14: # Skip if there isn't enough data for a seasonal model
                continue

            # Aggregate to Daily Average Signal Strength
            op_area_data = subset.groupby('date').agg({
                'signal_strength_dbm': 'mean',
                'is_weekend': 'max'
            }).asfreq('D').interpolate(method='time')

            # 4. Fit the SARIMAX Model (1,1,1)
            try:
                model = SARIMAX(
                    op_area_data['signal_strength_dbm'],
                    exog=op_area_data[['is_weekend']],
                    order=(1,1,1),
                    seasonal_order=(1,1,1,7),
                    enforce_stationarity=False,
                    enforce_invertibility=False
                )
                results = model.fit(disp=False)

                # 5. Residual Bootstrapping for Realistic Spikes
                historical_errors = results.resid.values[2:]

                # 6. Predict for 60 Days
                forecast_steps = 60
                future_dates = pd.date_range(start=op_area_data.index[-1] + pd.Timedelta(days=1), periods=forecast_steps)
                future_exog = pd.DataFrame({
                    'is_weekend': [1 if d.weekday() >= 5 else 0 for d in future_dates]
                }, index=future_dates)

                forecast_obj = results.get_forecast(steps=forecast_steps, exog=future_exog)
                forecast_smooth = forecast_obj.predicted_mean

                # Apply realistic fluctuations
                bootstrapped_noise = np.random.choice(historical_errors, size=forecast_steps, replace=True)
                realistic_values = forecast_smooth + bootstrapped_noise

                # 7. Build the Result Table
                forecast_df = pd.DataFrame({
                    'Date': future_dates.strftime('%Y-%m-%d'),
                    'Area': city,
                    'Operator': op,
                    'Predicted_Signal_dBm': realistic_values.values.round(2)
                })
                all_results.append(forecast_df)

            except Exception as e:
                print(f"Skipping {op} in {city} due to model error.")

    # 8. Combine and Export
    final_table = pd.concat(all_results, ignore_index=True)
    #final_table.to_csv("area_operator_60d_forecast.csv", index=False)

    print("\n" + "="*50)
    print("SUCCESS: Area-Based Forecast Complete")
    print("="*50)
    print(f"Total Rows Generated: {len(final_table)}")
    print("Sample Output (Next 5 Days for first Area/Operator):")
    print(final_table.head(5).to_string(index=False))
    print("="*50)
    print("Full results saved to: area_operator_60d_forecast.csv")

    return final_table

# Execute
if __name__ == "__main__":
    forecast_results = generate_area_operator_forecast(FILE_PATH)

from google.colab import data_table
import pandas as pd


df = pd.read_csv("area_operator_60d_forecast.csv")


data_table.enable_dataframe_formatter()


df

import pandas as pd
import numpy as np
from statsmodels.tsa.statespace.sarimax import SARIMAX
import joblib  # Preferred over pickle for ML models
import warnings
import os

# 1. Setup
warnings.filterwarnings('ignore')
FILE_PATH = "/content/telecom_33_towers_4_operators.csv"
MODEL_SAVE_PATH = "telecom_models_dictionary.pkl"

def train_and_save_models(path):
    # 2. Load Data
    try:
        df = pd.read_csv(path)
    except FileNotFoundError:
        print("Error: File not found.")
        return

    df['date'] = pd.to_datetime(df['date'])
    cities = df['city'].unique()
    operators = df['operator'].unique()

    # This dictionary will store models for every city and operator
    model_vault = {}

    print(f"Starting training and serialization for {len(cities) * len(operators)} potential models...")

    for city in cities:
        for op in operators:
            subset = df[(df['city'] == city) & (df['operator'] == op)]

            if len(subset) < 20: # Minimum data threshold
                continue

            # Aggregate to Daily Average Signal Strength
            op_area_data = subset.groupby('date').agg({
                'signal_strength_dbm': 'mean',
                'is_weekend': 'max'
            }).asfreq('D').interpolate(method='time')

            try:
                # 3. Fit Model
                model = SARIMAX(
                    op_area_data['signal_strength_dbm'],
                    exog=op_area_data[['is_weekend']],
                    order=(1,1,1),
                    seasonal_order=(1,1,1,7),
                    enforce_stationarity=False,
                    enforce_invertibility=False
                )
                results = model.fit(disp=False)

                # 4. Prepare data for the .pkl file
                # We save the model and the residues so the backend can "bootstrap" spikes
                model_entry = {
                    'model_results': results,
                    'historical_residuals': results.resid.values[2:],
                    'last_date': op_area_data.index[-1]
                }

                # Use a unique key for the dictionary
                model_key = f"{city}_{op}"
                model_vault[model_key] = model_entry

                print(f"Saved model for: {model_key}")

            except Exception as e:
                print(f"Skipping {op} in {city} due to training error.")

    # 5. Export the entire dictionary to a single .pkl file
    joblib.dump(model_vault, MODEL_SAVE_PATH)
    print("\n" + "="*50)
    print(f"SUCCESS: Saved {len(model_vault)} models to {MODEL_SAVE_PATH}")
    print("="*50)

if __name__ == "__main__":
    train_and_save_models(FILE_PATH)

# Loading example for your next task
loaded_models = joblib.load("telecom_models_dictionary.pkl")

# Access a specific model
my_model = loaded_models["Guntur_Airtel"]

# Generate a 7-day forecast
forecast = my_model['model_results'].get_forecast(steps=7, exog=some_exog_data)

import joblib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings

warnings.filterwarnings('ignore')

# 1. Load the model vault from the .pkl file
MODEL_FILE = "telecom_models_dictionary.pkl"

try:
    loaded_vault = joblib.load(MODEL_FILE)
    print(f"Successfully loaded {len(loaded_vault)} models from {MODEL_FILE}")
except FileNotFoundError:
    print("Error: .pkl file not found. Please run the training script first.")
    # Stop execution if file is missing
    raise

def test_model_prediction(city, operator, days=30):
    model_key = f"{city}_{operator}"

    if model_key not in loaded_vault:
        print(f"No model found for {model_key}. Available: {list(loaded_vault.keys())[:5]}...")
        return

    # 2. Extract the model data
    data = loaded_vault[model_key]
    model_results = data['model_results']
    historical_errors = data['historical_residuals']
    last_date = data['last_date']

    # 3. Prepare future external data (Exog)
    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=days)
    future_exog = pd.DataFrame({
        'is_weekend': [1 if d.weekday() >= 5 else 0 for d in future_dates]
    }, index=future_dates)

    # 4. Generate the Forecast
    forecast_obj = model_results.get_forecast(steps=days, exog=future_exog)
    forecast_smooth = forecast_obj.predicted_mean

    # Apply the "Realistic Spikes" using Bootstrapping from the saved residuals
    bootstrapped_noise = np.random.choice(historical_errors, size=days, replace=True)
    realistic_prediction = forecast_smooth + bootstrapped_noise

    # 5. Output Numeric Values (Sample)
    print(f"\n--- Testing Results for {city} | {operator} ---")
    results_df = pd.DataFrame({
        'Date': future_dates.strftime('%Y-%m-%d'),
        'Predicted_dBm': realistic_prediction.values.round(2)
    })
    print(results_df.head(10))

    # 6. Quick Visualization to verify it looks "Good Enough"
    plt.figure(figsize=(12, 5))
    plt.plot(future_dates, realistic_prediction, color='crimson', label='Model Prediction (from .pkl)')
    plt.plot(future_dates, forecast_smooth, color='black', linestyle='--', alpha=0.5, label='Underlying Trend')
    plt.title(f"Live Test: {model_key}")
    plt.ylabel("Signal Strength (dBm)")
    plt.legend()
    plt.show()

# --- RUN THE TEST ---
# Change these strings to any City and Operator you have in your dataset
test_model_prediction(city="Guntur", operator="Airtel", days=30)

import joblib
import pandas as pd
import numpy as np
import warnings

warnings.filterwarnings('ignore')

# 1. Load the model vault
MODEL_FILE = "telecom_models_dictionary.pkl"
loaded_vault = joblib.load(MODEL_FILE)

def get_full_60d_forecast(city, operator):
    model_key = f"{city}_{operator}"

    if model_key not in loaded_vault:
        print(f"No model found for {model_key}")
        return

    # 2. Extract model data
    data = loaded_vault[model_key]
    model_results = data['model_results']
    historical_errors = data['historical_residuals']
    last_date = data['last_date']

    # 3. Setup 60-day window
    days = 60
    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=days)
    future_exog = pd.DataFrame({
        'is_weekend': [1 if d.weekday() >= 5 else 0 for d in future_dates]
    }, index=future_dates)

    # 4. Generate Forecast
    forecast_obj = model_results.get_forecast(steps=days, exog=future_exog)
    forecast_smooth = forecast_obj.predicted_mean

    # Apply Bootstrapping for consistent fluctuations
    bootstrapped_noise = np.random.choice(historical_errors, size=days, replace=True)
    realistic_prediction = forecast_smooth + bootstrapped_noise

    # 5. Create the full Table
    results_df = pd.DataFrame({
        'Day': range(1, days + 1),
        'Date': future_dates.strftime('%Y-%m-%d'),
        'Predicted_Signal_dBm': realistic_prediction.values.round(2)
    })

    # 6. Force pandas to show all 60 rows
    pd.set_option('display.max_rows', None)

    print(f"\n--- FULL 60-DAY FORECAST: {city} | {operator} ---")
    print(results_df.to_string(index=False))

    # Reset display option to default
    pd.reset_option('display.max_rows')

# --- EXECUTE ---
get_full_60d_forecast(city="Guntur", operator="Airtel")

import joblib
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings

# 1. Setup
warnings.filterwarnings('ignore')
plt.style.use('ggplot')

# Path for your Colab file
MODEL_FILE = "telecom_models_dictionary.pkl"

def get_full_60d_forecast_with_graph(city, operator):
    # 2. Load the model vault
    try:
        loaded_vault = joblib.load(MODEL_FILE)
    except FileNotFoundError:
        print(f"Error: {MODEL_FILE} not found. Please run your training script first.")
        return

    model_key = f"{city}_{operator}"

    if model_key not in loaded_vault:
        print(f"No model found for {model_key}")
        return

    # 3. Extract the saved model metadata
    data = loaded_vault[model_key]
    model_results = data['model_results']
    historical_errors = data['historical_residuals']
    last_date = data['last_date']

    # 4. Generate the 60-day Timeline
    days = 60
    future_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=days)
    future_exog = pd.DataFrame({
        'is_weekend': [1 if d.weekday() >= 5 else 0 for d in future_dates]
    }, index=future_dates)

    # 5. Run Inference
    forecast_obj = model_results.get_forecast(steps=days, exog=future_exog)
    forecast_smooth = forecast_obj.predicted_mean
    conf_int = forecast_obj.conf_int()

    # Apply Residual Bootstrapping for identical fluctuations
    bootstrapped_noise = np.random.choice(historical_errors, size=days, replace=True)
    realistic_prediction = forecast_smooth + bootstrapped_noise

    # 6. Visualization
    plt.figure(figsize=(15, 6))

    # Plot the Realistic Forecast
    plt.plot(future_dates, realistic_pp0rediction, color='#c0392b',
             label='Realistic 60-Day Forecast (from .pkl)', linewidth=1.5)

    # Plot the underlying statistical trend
    plt.plot(future_dates, forecast_smooth, color='black',
             linestyle='--', label='Trend Logic', alpha=0.4)

    # Shade the Confidence Interval
    plt.fill_between(future_dates, conf_int.iloc[:, 0], conf_int.iloc[:, 1],
                     color='#c0392b', alpha=0.1, label='95% Probability Range')

    plt.title(f'60-Day Infrastructure Analysis: {city} | {operator}', fontsize=15)
    plt.ylabel('Signal Strength (dBm)')
    plt.xlabel('Date')
    plt.legend(loc='upper left')
    plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show()

    # 7. Print all 60 rows of numeric values
    results_df = pd.DataFrame({
        'Day': range(1, days + 1),
        'Date': future_dates.strftime('%Y-%m-%d'),
        'Predicted_Signal_dBm': realistic_prediction.values.round(2)
    })

    # Force Colab/Pandas to show all 60 rows
    pd.set_option('display.max_rows', None)
    print(f"\n--- FULL 60-DAY NUMERIC VALUES: {city} | {operator} ---")
    print(results_df.to_string(index=False))
    pd.reset_option('display.max_rows')

# --- RUN THE FINAL TEST ---
# You can change 'Guntur' and 'Airtel' to any other pair in your .pkl
get_full_60d_forecast_with_graph(city="Guntur", operator="Airtel")

